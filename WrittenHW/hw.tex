\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}
\graphicspath{ {Images/} }
\DeclareMathOperator{\Hcal}{\mathcal{H}}


\newcommand{\semester}{Spring 2018}
\newcommand{\assignmentId}{4}
\newcommand{\releaseDate}{11 April, 2018}
\newcommand{\dueDate}{23 April, 2018}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learining \semester}
\author{Homework \assignmentId}
\date{Handed out: \releaseDate\\
  Due date: \dueDate}

\begin{document}
\maketitle

\input{emacscomm}
\input{general-instructions}


\section{VC dimension}
\begin{enumerate}
\item~[25] Suppose we have a finite hypothesis space $H$%$\Hcal$.
\begin{enumerate}
%\item~[10] Suppose $|\Hcal| = 2^{10}$. What is the VC dimension of %$\Hcal$?
\item~[10] Suppose $|H| = 2^{10}$. What is the VC dimension of $H$?

    $\mathrm{VC}(H) <= \log_2(2^{10}) = 10$\\

%\item~[15] Generally, for  any finite $\Hcal$, what is $\mathrm{VC}(\Hcal)$ ?
\item~[15] Generally, for  any finite $H$, what is $\mathrm{VC}(H)$ ?

    $\mathrm{VC}(H) <= \log_2(|H|)$

\end{enumerate}
\item~[25] Prove that linear classifiers in a plane cannot shatter any $4$ distinct points.

    To show this we have to show that for any positioning of the points an adversarial can label the points in
    such a way that they cannot be shattered.

    In order to show we must show all the possible cases.

    \pagebreak
    \begin{enumerate}

    \item For case 1 where 3 points are in a line, and the $4th$ point can be anywhere. In this case when three points are in
    a line no matter where the other point is there is always a labeling that will not shatter the positioning.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=.5]{all_in_line.png}
    \end{figure}


    \item Then there is the second case where 3 points are not in a line, and there is two other cases that fall into
    this category.
        \begin{enumerate}
            \item Where the $4th$ point is anywhere inside a triangle of the other 3 points.
            \begin{figure}[h]
                \centering
                \includegraphics[scale=.5]{inside_triangle.png}
            \end{figure}
            \item Where the $4th$ point is anywhere outside of the triangle
            \begin{figure}[h]
                \centering
                \includegraphics[scale=.5]{not_in_triangle.png}
            \end{figure}
        \end{enumerate}
    \end{enumerate}

    Since all of these are all the possible configurations we can have of the points then we can say conclude that
    lenear classifires in a plane cannot shatter any 4 distinct points.



%\item~[20] Consider our infinite hypothesis space $\Hcal$ are all rectangles in a plain. Each rectangle corresponds to a classifier --- all the points inside the rectangle are classified as positive, and otherwise classified as negative. What is $\mathrm{VC}(\Hcal)$?
\item~[20] Consider our infinite hypothesis space $H$ are all rectangles in a plain. Each rectangle corresponds to a classifier --- all the points inside the rectangle are classified as positive, and otherwise classified as negative. What is $\mathrm{VC}(H)$?\\
Infinity
\end{enumerate}

\section{Support Vector Machines}
\begin{enumerate}
\item~[20 points] Prove that the objective function of soft SVM is convex,
\[
\frac{1}{2}\w^\top\w + C\sum_{i} \max\big(0, 1 - y_i(\w^\top\x_i+b)\big).
\]

\item~[10 points] Illustrate how the Occam's Razor's principle is reflected in SVM objective function and optimization. 


%calculate the subgradient
\item~[30 points] Suppose we have the following training dataset. We want to learn a SVM classifier. We initialize all the model parameters with $0$. We set the learning rates for the first three steps to $\{0.01, 0.005, 0.0025\}$.  Please list the sub-gradients of the SVM objective w.r.t the model parameters for the first three steps, when using the stochastic sub-gradient descent algorithm. 
\begin{table}[h]
        \centering
        \begin{tabular}{ccc|c}
        $x_1$ & $x_2$ & $x_3$ &  $y$\\ 
        \hline\hline
         $0.5$ & $-1$ & $0.3$ & $1$ \\ \hline
         $-1$ & $-2$ & $-2$ & $-1$\\ \hline
         $1.5$ & $0.2$ & $-2.5$ & $1$\\ \hline
        \end{tabular}
\end{table}

\end{enumerate}

\section{Programming Assignments}
\begin{enumerate}
\item We will implement  SVM with stochastic sub-gradient descent. We will reuse the dataset for Perceptron implementation. The features and labels are listed in the file ``classification/data-desc.txt''. The training data are stored in the file ``classification/train.csv'', consisting of $872$ examples. The test data are stored in ``classification/test.csv'', and comprise of $500$ examples. In both the training and testing datasets, feature values and labels are separated by commas. Set the maximum epochs $T$ to 50. Don't forget to shuffle the training examples at the start of each epoch. Use the curve of the objective function (along with the number of updates) to diagnosis the convergence. Try the hyperparameter $C$ from $\{\frac{10}{873}, \frac{100}{873}, \frac{300}{873}, \frac{500}{873,} \frac{700}{873}\}$. Don't forget to convert the labels to be in $\{1, -1\}$.  
\begin{enumerate}
\item~[50 points] Use the schedule of learning rate: $\gamma_t = \frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$. Please tune $\gamma_0$ and $d$ to ensure convergence. For each setting of $C$, report your test error.\\

$C = \frac{10}{873}$\\
Accuracy testing: $0.9639278557114228$\\
Test error = $0.03607214428857719\\ $


$C = \frac{100}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{300}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{500}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{700}{873}$\\
Accuracy testing: $0.9859719438877755$\\
Test error = $0.014028056112224463\\ $


\item~[50 points] Use the schedule $\gamma_t = \frac{\gamma_0}{1+t}$. Report the test error for each setting of C.

$C = \frac{10}{873}$\\
Accuracy testing: $0.9639278557114228$\\
Test error = $0.03607214428857719\\ $


$C = \frac{100}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{300}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{500}{873}$\\
Accuracy testing: $0.9879759519038076$\\
Test error = $0.012024048096192397\\ $


$C = \frac{700}{873}$\\
Accuracy testing: $0.9859719438877755$\\
Test error = $0.014028056112224463\\ $

\item~[20 points] For each $C$, report the differences between the model parameters learned from the two learning rate schedules, as well as the differences between the test errors. What can you conclude?


$C = \frac{10}{873}$\\
Model Parameters: $\frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$ and
$\frac{\gamma_0}{1+t}$

$[-25288.5887076, -15063.8240872, -16022.5625078, -2782.24411486, 27386.1415048]$\\
$[-21325.0217736, -10889.112699, -10492.8624735, -3512.49220633, 8911.99304415]$\\

$\frac{\gamma_0}{1+\frac{\gamma_0}{d}t} = 0.987975951904$\\
$\frac{\gamma_0}{1+t} = 0.963927855711$\\

$C = \frac{100}{873}$\\
Model Parameters: $\frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$ and
$\frac{\gamma_0}{1+t}$

$[-52441.4926675, -36188.5609245, -38209.6769276, -10145.7203947, 58579.8787385]$\\
$[-32492.0416935, -21117.4124267, -21532.6932631, -5031.71006657, 32825.5843357]$\\

$\frac{\gamma_0}{1+\frac{\gamma_0}{d}t} = 0.98997995992$\\
$\frac{\gamma_0}{1+t} = 0.987975951904$\\

$C = \frac{300}{873}$\\
Model Parameters: $\frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$ and
$\frac{\gamma_0}{1+t}$

$[-110346.392081, -75928.2314648, -78133.2954785, -27554.3258578, 104358.914249]$\\
$[-45287.1509343, -30057.5965674, -32806.9801795, -7789.72628667, 43844.6231866]$\\

$\frac{\gamma_0}{1+\frac{\gamma_0}{d}t} = 0.985971943888$\\
$\frac{\gamma_0}{1+t} = 0.987975951904$\\

$C = \frac{500}{873}$\\
Model Parameters: $\frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$ and
$\frac{\gamma_0}{1+t}$

$[-158098.660428, -112712.633892, -115071.502339, -50226.2395035, 143474.365839]$\\
$[-60703.5318481, -39702.8592759, -44171.35262, -12975.570726, 52167.76998]$\\

$\frac{\gamma_0}{1+\frac{\gamma_0}{d}t} = 0.985971943888$\\
$\frac{\gamma_0}{1+t} = 0.987975951904$\\

$C = \frac{700}{873}$\\
Model Parameters: $\frac{\gamma_0}{1+\frac{\gamma_0}{d}t}$ and
$\frac{\gamma_0}{1+t}$

$[-209124.451871, -144252.562456, -149288.400186, -59164.1727278, 185618.815655]$\\
$[-78860.6395292, -52367.748611, -57456.4070289, -19013.9104801, 61452.5135766]$\\

$\frac{\gamma_0}{1+\frac{\gamma_0}{d}t} = 0.985971943888$\\
$\frac{\gamma_0}{1+t} = 0.985971943888$\\


When $C$ is smaller the accuracy seemed to be less accurate, but as it got higher it did better.
We know that $C = \infinity$ overfits the data, meaning that in our case when $C$ got bigger it fit the data better.

\end{enumerate}
\end{enumerate}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
