# import numpy as np
#
# r = 0.1
# x = np.array([[1, -1, 2],
#               [1, 1, 3],
#               [-1, 1, 0],
#               [1, 2, -4],
#               [3, -1, -1]])
#
# y = np.array([[1], [4], [-1], [-2], [0]])
#
# w = np.transpose(np.zeros(3))
# b = np.zeros(1)
#
# m = x.shape[0]
#
# for i in range(m):
#
#     temW = np.zeros(w.shape[0])
#     for j in range(w.shape[0]):
#         print('$w\\big[{6}\\big] = {0} + {1}*({2} - {3} - {4})*{5}$\\\\'.
#               format(format(w[j], '.4f'), r, y[i][0], format(w.dot(x[i]), '.4f'), format(b[0], '.4f'), x[i][j], j))
#         temW[j] = w[j] + r * (y[i] - w.dot(x[i]) - b) * x[i][j]
#
#     print('$\w = {0}$\\\\'.format(temW))
#
#     print('$b = {0} + {1}*({2} - {3} - {4})$\\\\'.format(format(b[0], '.4f'), r, format(y[i][0], '.4f'),
#                                                    format(w.dot(x[i]), '.4f'), format(b[0], '.4f')))
#     b = b + r * (y[i] - w.dot(x[i]) - b)
#     print('$b = {0}$\\\\'.format(format(b[0], '.4f')))
#     print()
#
#     w = temW

import numpy as np
import random
import sklearn
from sklearn.datasets.samples_generator import make_regression
import pylab
import pandas as pd
import sys

def rt0(r0, t, C):
    denom = (r0*t)/C
    return (r0)/(1 + denom)

def rt1(r0, t, C):
    return (r0)/(1 + t)


def StochasticGradientDescent(x, y, r, rFunc, d, C, T):
    w = np.transpose(np.zeros(x.shape[1]))
    N = x.shape[0]

    indices = np.array([i for i in range(0, N)])
    for t in range(T):
        np.random.shuffle(indices)

        for i in indices:

            if y[i] * (w.dot(x[i])) <= 1:
                w = (1 - r)*(w) + rFunc(r, t, d)*(C * N * y[i] * x[i])
            else:
                w = (1 - r)*w
    return w

def predict(D, w):
    n = D.shape[1] - 1
    D = D.as_matrix()
    errorSum = 0
    x = D[:, :n]
    y = D[:, n]

    for i in range(D.shape[0]):

        # error = y[i]*(w.T.dot(x[i]))
        # error = y[i] * (w.dot(x[i]))

        # if error < 0:
        #     errorSum += 1

        e = y[i] * (w.T.dot(x[i]))
        if y[i] * (w.dot(x[i])) <= 0:
            errorSum += 1


    # print('shape = {0}'.format(errorSum))
    # print('shape = {0}'.format(D.shape[0]))
    # print (errorSum)
    # print(D.shape[0])
    return errorSum/D.shape[0]

if __name__ == '__main__':
    print('*'*22 + 'SVM' + '*'*25)
    trainFilename = sys.argv[1]
    testFilename = sys.argv[2]

    # x = np.array([[1, -1, 2],
    #               [1, 1, 3],
    #               [-1, 1, 0],
    #               [1, 2, -4],
    #               [3, -1, -1]])
    #
    # y = np.array([[1], [4], [-1], [-2], [0]])
    #
    # x = np.array([[0.5, -1, 0.3],
    #               [-1, -2, -2],
    #               [1.5, 0.2, -2.5]])
    #
    # y = np.array([[1], [-1], [1]])

    # dt = pd.read_csv(trainFilename)
    dt = pd.read_csv('../dataset-hw2/classification/train.csv')
    dt.columns = ['x1', 'x2', 'x3', 'x4', 'y']
    dt['b'] = 1
    xpd = dt.loc[:, ['x1', 'x2', 'x3', 'x4']]
    # xpd = dt.loc[:, ['b','x1', 'x2', 'x3', 'x4']]
    dt.loc[dt['y'] == 0] = -1
    ypd = dt.loc[:,['y']]


    x = xpd.as_matrix()
    y = ypd.values

    r = (1/2)**7#(64) # learning rate
    ep = 1/1000000 # convergence criteria


    C = [10/873, 100/873, 300/873, 500/873, 700/873]
    T = 50
    d = (1/2)**8

    w = StochasticGradientDescent(x, y, r, rt0, d, C[0], T)

    print (('w = {0}').format(w))

    # dt = pd.read_csv(sys.argv[2])
    # dt = pd.read_csv('../dataset-hw2/regression/train.csv')

    # dt.columns = ['x1', 'x2', 'x3', 'x4', 'y']
    # dt['b'] = 1
    # xpd = dt.loc[:, ['b','x1', 'x2', 'x3', 'x4', 'y']]
    # ypd = dt.loc[:,['y']]

    # testD = pd.read_csv(testFilename)
    testD = pd.read_csv('../dataset-hw2/classification/test.csv')
    testD.columns = ['x1', 'x2', 'x3', 'x4', 'y']
    testD.loc[testD['y'] == 0] = -1
    testD['b'] = 1
    testD = testD.loc[:, ['x1', 'x2', 'x3', 'x4', 'y']]
    # testD = testD.loc[:, ['b','x1', 'x2', 'x3', 'x4', 'y']]


    print(1 - predict(testD, w))

    # x = xpd.as_matrix()
    # y = ypd.values
    # print('Final Cost Funtion = {0}'.format((1/2) *
    # sum([ (y[i] - w.dot(x[i]) - b)**2 for i in range(x.shape[0]) ])))

